{
    "componentChunkName": "component---src-pages-labs-lab-1-mdx",
    "path": "/Labs/Lab1/",
    "result": {"pageContext":{"frontmatter":{"0":"e","1":"x","2":"p","3":"o","4":"r","5":"t","6":" ","7":"c","8":"o","9":"n","10":"s","11":"t","12":" ","13":"T","14":"i","15":"t","16":"l","17":"e","18":" ","19":"=","20":" ","21":"(","22":")","23":" ","24":"=","25":">","26":" ","27":"(","28":" ","29":"<","30":"s","31":"p","32":"a","33":"n","34":">","35":" ","36":"L","37":"a","38":"b","39":" ","40":"1","41":" ","42":"-","43":" ","44":"N","45":"P","46":"M","47":" ","48":"I","49":"n","50":"t","51":"r","52":"o","53":" ","54":"<","55":"b","56":"r","57":" ","58":"/","59":">","60":" ","61":"<","62":"/","63":"s","64":"p","65":"a","66":"n","67":">","68":" ","69":")","70":";","description":"Labs / Lab 1","title":"Labs / Lab 1"},"relativePagePath":"/Labs/Lab1.mdx","titleType":"page","MdxNode":{"id":"4594ffca-9f7d-540c-93de-340330c8e540","children":[],"parent":"617598c8-a1d2-5bef-b8e8-d32f6de1a310","internal":{"content":"---\nexport const Title = () => (\n  <span>\n    Lab 1 - NPM Intro <br />\n  </span>\n);\n---\n\n## Lab Overview\n\nThis lab demonstrates how workflows works and how they can be used to go from a high level\nreport to a more granular one, allowing us to troubleshoot problems in a much more efficient way.\n\n## Setting the Stage\n\nWe have a look at the most severe alerts that are active, and we see a big drop of VPN connections\non one of the firewalls. We try to correlate the drop of connections with other metrics, and we find out\nthat there is a correlation between drop of connections and high CPU. Reviewing the top processes\nusing CPU we identify that the SSL process that is the one having issues and using data analytics we\nrealize this is not something normal (baseline). Finally reviewing the flows (using metric to flow) we\nidentify a big surge of TCP SYN connections (potential DDoS attack) that is generating a high CPU\nload on the SSL process, that is impacting on the VPN connections. Hence, we have identified the\nroot cause of the issue and the IP addresses generating the issue, therefore we can configure our\nfirewall or IPS/IDS to block those connection attempts.\n\n## Step 0:\n\nLogin to the SevOne lab environment as referenced in the SevOne Lab Environment Tab\n\nFull Demo Video Located in the Uncut Lab Videos Tab\n\n**Video For Steps 1 and 2:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf1.mp4\"/>\n</video>\n\n## Step 1: \n\nWe will go through an example process of diagnosing high cpu usage. \n\nSearch for Network Performance report in the top right.\n\n![Lab 1](images/1.1.png)\n\nNavigate to Active Alerts.\n\nMake sure the time period is set to Past 48 Hours as shown here.\n\n![Lab 1](images/1.2.png)\n\n## Step 2:\n\nGo to device groups in the top left.\n\nSearch for P1 as shown below\n\n![Lab 1](images/1.3.png)\n\nNow we can check out the most severe alerts.\n\nLook for the Chicago Firewall alert and click on the CISCO summary.\n\n![Lab 1](images/1.4.png)\n\n**Video For Steps 3 and 4:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf2.mp4\"/>\n</video>\n\n## Step 3:\n\nNavigate to the connections tab and notice the drops on the graph as shown here.\n\n![Lab 1](images/1.5.png)\n\nCheck out some of the other tabs until you get to the CPU window. \n\nNotice that the cpu has similar spikes as seen before. \n\nClick on the graph spike and navigate to the Instant Graphs Workspace.\n\n![Lab 1](images/1.6.png)\n\n## Step 4:\n\nOnce you see the CPU screen add the connections object like so.\n\n![Lab 1](images/1.7.png)\n\nEverything will go blank, just add the 5 minute average and current connections as indicators.\n\n![Lab 1](images/1.8.png)\n\nNow we can see the correlation between a drop in connections and high cpu usage.\n\n![Lab 1](images/1.9.png)\n\n\n**Video For Steps 5 and 6:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf3.mp4\"/>\n</video>\n\n## Step 5:\n\nNavigate back a page. \n\n![Lab 1](images/1.10.png)\n\nThen move over to the processes tab.\n\n![Lab 1](images/1.11.png)\n\n## Step 6: \n\nOrder the CPU Time graph by max to check which process uses the most time. \n\n![Lab 1](images/1.12.png)\n\nClick on the average CPU time for the SSL process\n\n![Lab 1](images/1.13.png)\n\n**Video For Step 7:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf4.mp4\"/>\n</video>\n\n## Step 7: \n\nIt is evident that the spikes on the graph are not normal.\n\n![Lab 1](images/1.14.png)\n\nSo now that we know that high cpu is generated from the SSL processes. We can\ngoogle some possible reasons for this.\n\nEnter the following into google chrome.\n\n![Lab 1](images/1.15.png)\n\nAfter clicking on the first link we get this answer:\n\n![Lab 1](images/1.16.png)\n\nSo we can see that SYN attacks could be the cause for high cpu.\n\nLet's investigate further.\n\n\n**Video For Step 8:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf5.mp4\"/>\n</video>\n\n## Step 8:\n\nNavigate to the Flow tab at the very right.\n\n![Lab 1](images/1.17.png)\n\nAt the bottom we have a Denial of Service graph. Where we can see\na large amount of TCP/SYN packets.\n\n![Lab 1](images/1.18.png)\n\nSo we have found the culprit.\n\n## Step 9:\n\nTo make sure we have the right answer, navigate to the connections tab.\n\n![Lab 1](images/1.19.png)\n\nThen configure the graph as shown below.\n\n![Lab 1](images/1.20.png)\n\nWe can now see a direct correlation between the spikes and drops on both graphs.\n\n![Lab 1](images/1.21.png)\n\nSo we have determined the cause of high cpu usage.\n\n## Summary/Additional Questions:\nInstructor-Led Discussion","type":"Mdx","contentDigest":"8efa20750b4487837b11642dc1e74d3d","owner":"gatsby-plugin-mdx","counter":1493},"frontmatter":{"0":"e","1":"x","2":"p","3":"o","4":"r","5":"t","6":" ","7":"c","8":"o","9":"n","10":"s","11":"t","12":" ","13":"T","14":"i","15":"t","16":"l","17":"e","18":" ","19":"=","20":" ","21":"(","22":")","23":" ","24":"=","25":">","26":" ","27":"(","28":" ","29":"<","30":"s","31":"p","32":"a","33":"n","34":">","35":" ","36":"L","37":"a","38":"b","39":" ","40":"1","41":" ","42":"-","43":" ","44":"N","45":"P","46":"M","47":" ","48":"I","49":"n","50":"t","51":"r","52":"o","53":" ","54":"<","55":"b","56":"r","57":" ","58":"/","59":">","60":" ","61":"<","62":"/","63":"s","64":"p","65":"a","66":"n","67":">","68":" ","69":")","70":";","description":"Labs / Lab 1","title":"Labs / Lab 1"},"exports":{},"rawBody":"---\nexport const Title = () => (\n  <span>\n    Lab 1 - NPM Intro <br />\n  </span>\n);\n---\n\n## Lab Overview\n\nThis lab demonstrates how workflows works and how they can be used to go from a high level\nreport to a more granular one, allowing us to troubleshoot problems in a much more efficient way.\n\n## Setting the Stage\n\nWe have a look at the most severe alerts that are active, and we see a big drop of VPN connections\non one of the firewalls. We try to correlate the drop of connections with other metrics, and we find out\nthat there is a correlation between drop of connections and high CPU. Reviewing the top processes\nusing CPU we identify that the SSL process that is the one having issues and using data analytics we\nrealize this is not something normal (baseline). Finally reviewing the flows (using metric to flow) we\nidentify a big surge of TCP SYN connections (potential DDoS attack) that is generating a high CPU\nload on the SSL process, that is impacting on the VPN connections. Hence, we have identified the\nroot cause of the issue and the IP addresses generating the issue, therefore we can configure our\nfirewall or IPS/IDS to block those connection attempts.\n\n## Step 0:\n\nLogin to the SevOne lab environment as referenced in the SevOne Lab Environment Tab\n\nFull Demo Video Located in the Uncut Lab Videos Tab\n\n**Video For Steps 1 and 2:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf1.mp4\"/>\n</video>\n\n## Step 1: \n\nWe will go through an example process of diagnosing high cpu usage. \n\nSearch for Network Performance report in the top right.\n\n![Lab 1](images/1.1.png)\n\nNavigate to Active Alerts.\n\nMake sure the time period is set to Past 48 Hours as shown here.\n\n![Lab 1](images/1.2.png)\n\n## Step 2:\n\nGo to device groups in the top left.\n\nSearch for P1 as shown below\n\n![Lab 1](images/1.3.png)\n\nNow we can check out the most severe alerts.\n\nLook for the Chicago Firewall alert and click on the CISCO summary.\n\n![Lab 1](images/1.4.png)\n\n**Video For Steps 3 and 4:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf2.mp4\"/>\n</video>\n\n## Step 3:\n\nNavigate to the connections tab and notice the drops on the graph as shown here.\n\n![Lab 1](images/1.5.png)\n\nCheck out some of the other tabs until you get to the CPU window. \n\nNotice that the cpu has similar spikes as seen before. \n\nClick on the graph spike and navigate to the Instant Graphs Workspace.\n\n![Lab 1](images/1.6.png)\n\n## Step 4:\n\nOnce you see the CPU screen add the connections object like so.\n\n![Lab 1](images/1.7.png)\n\nEverything will go blank, just add the 5 minute average and current connections as indicators.\n\n![Lab 1](images/1.8.png)\n\nNow we can see the correlation between a drop in connections and high cpu usage.\n\n![Lab 1](images/1.9.png)\n\n\n**Video For Steps 5 and 6:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf3.mp4\"/>\n</video>\n\n## Step 5:\n\nNavigate back a page. \n\n![Lab 1](images/1.10.png)\n\nThen move over to the processes tab.\n\n![Lab 1](images/1.11.png)\n\n## Step 6: \n\nOrder the CPU Time graph by max to check which process uses the most time. \n\n![Lab 1](images/1.12.png)\n\nClick on the average CPU time for the SSL process\n\n![Lab 1](images/1.13.png)\n\n**Video For Step 7:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf4.mp4\"/>\n</video>\n\n## Step 7: \n\nIt is evident that the spikes on the graph are not normal.\n\n![Lab 1](images/1.14.png)\n\nSo now that we know that high cpu is generated from the SSL processes. We can\ngoogle some possible reasons for this.\n\nEnter the following into google chrome.\n\n![Lab 1](images/1.15.png)\n\nAfter clicking on the first link we get this answer:\n\n![Lab 1](images/1.16.png)\n\nSo we can see that SYN attacks could be the cause for high cpu.\n\nLet's investigate further.\n\n\n**Video For Step 8:**\n<video controls poster=\"/images/VideoCover.png\" style={{ width: `50%`, height: `50%`}}>\n  <source src=\"/videos/NPM/NetPerf5.mp4\"/>\n</video>\n\n## Step 8:\n\nNavigate to the Flow tab at the very right.\n\n![Lab 1](images/1.17.png)\n\nAt the bottom we have a Denial of Service graph. Where we can see\na large amount of TCP/SYN packets.\n\n![Lab 1](images/1.18.png)\n\nSo we have found the culprit.\n\n## Step 9:\n\nTo make sure we have the right answer, navigate to the connections tab.\n\n![Lab 1](images/1.19.png)\n\nThen configure the graph as shown below.\n\n![Lab 1](images/1.20.png)\n\nWe can now see a direct correlation between the spikes and drops on both graphs.\n\n![Lab 1](images/1.21.png)\n\nSo we have determined the cause of high cpu usage.\n\n## Summary/Additional Questions:\nInstructor-Led Discussion","fileAbsolutePath":"/Users/brandonwu/Documents/PTS/Gatsby/SevOne-1-Day-Partner-Workshop/src/pages/Labs/Lab1.mdx"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}