{
    "componentChunkName": "component---src-pages-labs-lab-1-mdx",
    "path": "/Labs/Lab1/",
    "result": {"pageContext":{"frontmatter":{"0":"e","1":"x","2":"p","3":"o","4":"r","5":"t","6":" ","7":"c","8":"o","9":"n","10":"s","11":"t","12":" ","13":"T","14":"i","15":"t","16":"l","17":"e","18":" ","19":"=","20":" ","21":"(","22":")","23":" ","24":"=","25":">","26":" ","27":"(","28":" ","29":"<","30":"s","31":"p","32":"a","33":"n","34":">","35":" ","36":"L","37":"a","38":"b","39":" ","40":"1","41":" ","42":"-","43":" ","44":"N","45":"e","46":"t","47":"w","48":"o","49":"r","50":"k","51":" ","52":"P","53":"e","54":"r","55":"f","56":"o","57":"r","58":"m","59":"a","60":"n","61":"c","62":"e","63":" ","64":"<","65":"b","66":"r","67":" ","68":"/","69":">","70":" ","71":"<","72":"/","73":"s","74":"p","75":"a","76":"n","77":">","78":" ","79":")","80":";","description":"Labs / Lab 1","title":"Labs / Lab 1"},"relativePagePath":"/Labs/Lab1.mdx","titleType":"page","MdxNode":{"id":"4594ffca-9f7d-540c-93de-340330c8e540","children":[],"parent":"617598c8-a1d2-5bef-b8e8-d32f6de1a310","internal":{"content":"---\nexport const Title = () => (\n  <span>\n    Lab 1 - Network Performance <br />\n  </span>\n);\n---\n\n## Link to the Video: \nhttps://ibm.box.com/s/jhba3faymuyuabfvws7qzqfqy48613h8\n\n\n\n## Step 1: \n\nWe will go through an example process of diagnosing high cpu usage. \n\nMake your way to demo.sevone.com.\n\nThen search for Network Performance report in the top right.\n\n![Lab 1](images/1.1.png)\n\nNavigate to Active Alerts.\n\nMake sure the time period is set to Past 48 Hours as shown here.\n\n![Lab 1](images/1.2.png)\n\n## Step 2:\n\nGo to device groups in the top left.\n\nSearch for P1 as shown below\n\n![Lab 1](images/1.3.png)\n\nNow we can check out the most severe alerts.\n\nLook for the Chicago Firewall alert and click on the CISCO summary.\n\n![Lab 1](images/1.4.png)\n\n## Step 3:\n\nNavigate to the connections tab and notice the drops on the graph as shown here.\n\n![Lab 1](images/1.5.png)\n\nCheck out some of the other tabs until you get to the CPU window. \n\nNotice that the cpu has similar spikes as seen before. \n\nClick on the graph spike and navigate to the Instant Graphs Workspace.\n\n![Lab 1](images/1.6.png)\n\n## Step 4:\n\nOnce you see the CPU screen add the connections object like so.\n\n![Lab 1](images/1.7.png)\n\nEverything will go blank, just add the 5 minute average and current connections as indicators.\n\n![Lab 1](images/1.8.png)\n\nNow we can see the correlation between a drop in connections and high cpu usage.\n\n![Lab 1](images/1.9.png)\n\n## Step 5:\n\nNavigate back a page. \n\n![Lab 1](images/1.10.png)\n\nThen move over to the processes tab.\n\n![Lab 1](images/1.11.png)\n\n## Step 6: \n\nOrder the CPU Time graph by max to check which process uses the most time. \n\n![Lab 1](images/1.12.png)\n\nClick on the average CPU time for the SSL process\n\n![Lab 1](images/1.13.png)\n\n## Step 7: \n\nIt is evident that the spikes on the graph are not normal.\n\n![Lab 1](images/1.14.png)\n\nSo now that we know that high cpu is generated from the SSL processes. We can\ngoogle some possible reasons for this.\n\nEnter the following into google chrome.\n\n![Lab 1](images/1.15.png)\n\nAfter clicking on the first link we get this answer:\n\n![Lab 1](images/1.16.png)\n\nSo we can see that SYN attacks could be the cause for high cpu.\n\nLet's investigate further.\n\n## Step 8:\n\nNavigate to the Flow tab at the very right.\n\n![Lab 1](images/1.17.png)\n\nAt the bottom we have a Denial of Service graph. Where we can see\na large amount of TCP/SYN packets.\n\n![Lab 1](images/1.18.png)\n\nSo we have found the culprit.\n\n## Step 9:\n\nTo make sure we have the right answer, navigate to the connections tab.\n\n![Lab 1](images/1.19.png)\n\nThen configure the graph as shown below.\n\n![Lab 1](images/1.20.png)\n\nWe can now see a direct correlation between the spikes and drops on both graphs.\n\n![Lab 1](images/1.21.png)\n\nSo we have determined the cause of high cpu usage.\n\n\n","type":"Mdx","contentDigest":"b168276527566177e04f8220ffd10c12","owner":"gatsby-plugin-mdx","counter":208},"frontmatter":{"0":"e","1":"x","2":"p","3":"o","4":"r","5":"t","6":" ","7":"c","8":"o","9":"n","10":"s","11":"t","12":" ","13":"T","14":"i","15":"t","16":"l","17":"e","18":" ","19":"=","20":" ","21":"(","22":")","23":" ","24":"=","25":">","26":" ","27":"(","28":" ","29":"<","30":"s","31":"p","32":"a","33":"n","34":">","35":" ","36":"L","37":"a","38":"b","39":" ","40":"1","41":" ","42":"-","43":" ","44":"N","45":"e","46":"t","47":"w","48":"o","49":"r","50":"k","51":" ","52":"P","53":"e","54":"r","55":"f","56":"o","57":"r","58":"m","59":"a","60":"n","61":"c","62":"e","63":" ","64":"<","65":"b","66":"r","67":" ","68":"/","69":">","70":" ","71":"<","72":"/","73":"s","74":"p","75":"a","76":"n","77":">","78":" ","79":")","80":";","description":"Labs / Lab 1","title":"Labs / Lab 1"},"exports":{},"rawBody":"---\nexport const Title = () => (\n  <span>\n    Lab 1 - Network Performance <br />\n  </span>\n);\n---\n\n## Link to the Video: \nhttps://ibm.box.com/s/jhba3faymuyuabfvws7qzqfqy48613h8\n\n\n\n## Step 1: \n\nWe will go through an example process of diagnosing high cpu usage. \n\nMake your way to demo.sevone.com.\n\nThen search for Network Performance report in the top right.\n\n![Lab 1](images/1.1.png)\n\nNavigate to Active Alerts.\n\nMake sure the time period is set to Past 48 Hours as shown here.\n\n![Lab 1](images/1.2.png)\n\n## Step 2:\n\nGo to device groups in the top left.\n\nSearch for P1 as shown below\n\n![Lab 1](images/1.3.png)\n\nNow we can check out the most severe alerts.\n\nLook for the Chicago Firewall alert and click on the CISCO summary.\n\n![Lab 1](images/1.4.png)\n\n## Step 3:\n\nNavigate to the connections tab and notice the drops on the graph as shown here.\n\n![Lab 1](images/1.5.png)\n\nCheck out some of the other tabs until you get to the CPU window. \n\nNotice that the cpu has similar spikes as seen before. \n\nClick on the graph spike and navigate to the Instant Graphs Workspace.\n\n![Lab 1](images/1.6.png)\n\n## Step 4:\n\nOnce you see the CPU screen add the connections object like so.\n\n![Lab 1](images/1.7.png)\n\nEverything will go blank, just add the 5 minute average and current connections as indicators.\n\n![Lab 1](images/1.8.png)\n\nNow we can see the correlation between a drop in connections and high cpu usage.\n\n![Lab 1](images/1.9.png)\n\n## Step 5:\n\nNavigate back a page. \n\n![Lab 1](images/1.10.png)\n\nThen move over to the processes tab.\n\n![Lab 1](images/1.11.png)\n\n## Step 6: \n\nOrder the CPU Time graph by max to check which process uses the most time. \n\n![Lab 1](images/1.12.png)\n\nClick on the average CPU time for the SSL process\n\n![Lab 1](images/1.13.png)\n\n## Step 7: \n\nIt is evident that the spikes on the graph are not normal.\n\n![Lab 1](images/1.14.png)\n\nSo now that we know that high cpu is generated from the SSL processes. We can\ngoogle some possible reasons for this.\n\nEnter the following into google chrome.\n\n![Lab 1](images/1.15.png)\n\nAfter clicking on the first link we get this answer:\n\n![Lab 1](images/1.16.png)\n\nSo we can see that SYN attacks could be the cause for high cpu.\n\nLet's investigate further.\n\n## Step 8:\n\nNavigate to the Flow tab at the very right.\n\n![Lab 1](images/1.17.png)\n\nAt the bottom we have a Denial of Service graph. Where we can see\na large amount of TCP/SYN packets.\n\n![Lab 1](images/1.18.png)\n\nSo we have found the culprit.\n\n## Step 9:\n\nTo make sure we have the right answer, navigate to the connections tab.\n\n![Lab 1](images/1.19.png)\n\nThen configure the graph as shown below.\n\n![Lab 1](images/1.20.png)\n\nWe can now see a direct correlation between the spikes and drops on both graphs.\n\n![Lab 1](images/1.21.png)\n\nSo we have determined the cause of high cpu usage.\n\n\n","fileAbsolutePath":"/Users/brandonwu/Documents/PTS/Gatsby/SevOne-1-Day-Partner-Workshop/src/pages/Labs/Lab1.mdx"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}